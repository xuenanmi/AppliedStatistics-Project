---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2020, Xuenan Mi, NetID: xmi4"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---


***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
library(MASS)
?cats
cat_model<- lm(Hwt ~ Bwt, data= cats)
summary(cat_model)
summary(cat_model)$coefficients [2,3]
summary(cat_model)$coefficients [2,4]
```

- The null hypothesis: $\beta_1 = 0$, which indicates there is no significant linear relationship between cat's height weight and body weight; The alternative hypothesis: $\beta_1 \neq 0$, which indicates there is significant linear relationship between cat's height weight and body weight.
- The test statistic is $t = 16.11939$
- The P-value is $6.969045*10^{-34}$
- Decision at $\alpha = 0.05$ : Reject $H_0$.
- Conclusion: There is signifcant linear relationship between cat's height weight and body weight.


**(b)** Calculate a 95% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, parm = "Bwt", level = 0.95)
```

95% CI for $\beta_1$ is $(3.539343, 4.528782)$ .

We are 95% confident that given 1kg increase in cat's body weight, the average increase in cat's heart weight will be between 3.539343 and 4.528782 gram.

**(c)** Calculate a 90% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, parm = "(Intercept)", level = 0.90)
```

90% CI for $\beta_0$ is $(-1.502834, 0.7895096)$ .

We are 90% confident that when cat's body weight is zero, the average cat's heart weight will be between -1.502834 and 0.7895096 gram. Actually, the negative value of cat's heart weight is not reasonable. 

**(d)** Use a 90% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

```{r}
new_bwt = data.frame (Bwt = c(2.1, 2.8))
pred_level = predict(cat_model, newdata = new_bwt, interval = c('confidence'), level = 0.9)
pred_level
```

90% CI of mean heart weight for body weight of 2.1kg is $(7.787882, 8.441856)$.

90% CI of mean heart weight for body weight of 2.8kg is $(10.735843, 11.141583)$.

```{r}
diff(pred_level[1,2:3])
diff(pred_level[2,2:3])
mean(cats$Bwt)
```

The 90% CI for body weight of 2.1 kg is wider. Because 2.1 is further away from the sample mean of body weight.


**(e)** Use a 90% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

```{r}
new_bwt = data.frame (Bwt = c(2.8, 4.2))
predict(cat_model, newdata = new_bwt, interval = c('prediction'), level = 0.9)

```

90% prediction interval to predict heart weight for body weight of 2.8kg is $(8.525541, 13.35189)$.

90% prediction interval to predict heart weight for body weight of 2.8kg is $(14.097100, 19.07570)$.


**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.

```{r}
Bwt_grid = seq (min(cats$Bwt), max(cats$Bwt), 0.01)
Hwt_ci = predict (cat_model, newdata = data.frame(Bwt = Bwt_grid), interval = "confidence", level = 0.95)
Hwt_pi = predict (cat_model, newdata = data.frame(Bwt = Bwt_grid), interval = "prediction", level = 0.95)
plot (Hwt ~ Bwt, data = cats,
      main = "Height weight vs Body weight",
      xlab = "Body weight (Kg)",
      ylab = "Heart weight (g)",
      col = "grey",
      cex = 1,
      pch = 20,
      ylim = c(min(Hwt_pi), max(Hwt_pi))
      )
abline (cat_model, col = 'black',lwd = 3)
lines(Bwt_grid,Hwt_ci[,2], col = 'red', lwd =2, lty = 2)
lines(Bwt_grid,Hwt_ci[,3], col = 'red', lwd =2, lty = 2)
lines(Bwt_grid,Hwt_pi[,2], col = 'blue', lwd =2, lty = 3)
lines(Bwt_grid,Hwt_pi[,3], col = 'blue', lwd =2, lty = 3)
legend ("topleft", c("Regression line", "95% Confidence bands", "95% Prediction bands"), cex = 0.7, col = c('black', 'red', 'blue') , lty = c(1,2,3))
points(mean(cats$Bwt), mean(cats$Hwt), pch = "+", cex =3)
```


**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
coeff = summary(cat_model)$coefficients
t_stats = (coeff [2, 1] - 4) / (coeff [2, 2])
t_stats
pval = 2*pt(-abs(t_stats), df = nrow(cats)-2 )
pval
```

- Test statistics : `r t_stats`
- p-value: `r pval`
- Decision: Because the p-value is larger than $\alpha = 0.05$, we fail to reject the $H_0: \beta_1 = 4$


***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
ozone_wind_model = lm(ozone ~ wind, data = Ozone)
summary (ozone_wind_model)
summary (ozone_wind_model)$coefficients[2,3]
summary (ozone_wind_model)$coefficients[2,4]
```

- The null hypothesis: $\beta_1 = 0$, which indicates there is no significant relationship between ozone and wind; The alternative hypothesis; $\beta_1 \neq 0$, which indicates there is significant relationship between ozone and wind.
- The test statistic is $t = -0.2189811$
- The P-value is $0.8267954$
- Decision at $\alpha = 0.01$ : Fail to reject $H_0$.
- Conclusion: There is no signifcant linear relationship between Daily maximum one-hour-average ozone reading and wind speed.


**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
ozone_temp_model = lm(ozone ~ temp, data = Ozone)
summary (ozone_temp_model)
summary (ozone_temp_model)$coefficients[2,3]
summary (ozone_temp_model)$coefficients[2,4]
```

- The null hypothesis: $\beta_1 = 0$, which indicates there is no significant relationship between ozone and temperature; The alternative hypothesis: $\beta_1 \neq 0$,, which indicates there is significant relationship between ozone and temperature.
- The test statistic is $t = 22.84896$
- The P-value is $8.153764*10^{-71}$
- Decision at $\alpha = 0.01$ : Reject $H_0$.
- Conclusion: There is signifcant linear relationship between Daily maximum one-hour-average ozone reading and temperature.


***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 722
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)
```

```{r}
sim_slr = function (x, beta_0, beta_1, sigma){
  epsilon = rnorm (n, 0, sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response =y)
}
beta_hat_0 = c(rep(0, 2000))
beta_hat_1 = c(rep(0, 2000))
for (i in 1:length(beta_hat_0)){
  df = sim_slr(x,-5, 3.25, 4)
  model = lm (response ~ predictor, data = df)
  beta_hat_1[i] = coef(model)[2]
  beta_hat_0[i] = coef(model)[1]
}
```


**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

\[
  \hat{\beta}_1\sim N\left(\beta_1, \sigma^2/S_{xx}\right)
\]

\[
  \hat{\beta_0}\sim N\left(\beta_0, \sigma^2(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}) \right)
\]
```{r}
library(knitr)
beta_0 = -5
beta_1 = 3.25
sigma = 4
Sxx = sum((x-mean(x))^2)
sd_beta_1 = sigma/sqrt(Sxx)
sd_beta_0 = sigma*sqrt(1/n + (mean(x))^2/Sxx)

df_table = data.frame(
  Value = c('True mean value', 'Simulated mean value','True standard deviation','Simulated standard deviation'),
  hat_beta_0 = c(beta_0,mean(beta_hat_0),sd_beta_0,sd(beta_hat_0)),
  hat_beta_1 = c(beta_1,mean(beta_hat_1),sd_beta_1,sd(beta_hat_1))
)
kable(df_table)
```


**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.

```{r}
par(mfrow = c(1,2))
hist(beta_hat_0, breaks = 20, col = 'dodgerblue', xlab = expression(hat(beta)[0]), main = '',prob = TRUE)
curve(dnorm(x, beta_0, sd_beta_0), add = TRUE, lwd =3)

hist(beta_hat_1, breaks = 20, col = 'dodgerblue', xlab = expression(hat(beta)[1]), main = '',prob = TRUE)
curve(dnorm(x, beta_1, sd_beta_1), add = TRUE, lwd =3)
```


***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 722
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)
```

```{r}
sim_slr = function (x, beta_0, beta_1, sigma){
  epsilon = rnorm (n, 0, sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response =y)
}
beta_hat_1 = c(rep(0, 2500))
s_e = c(rep(0, 2500))
for (i in 1:length(beta_hat_1)){
  df = sim_slr(x,5, 2, 3)
  model = lm (response ~ predictor, data = df)
  beta_hat_1[i] = coef(model)[2]
  s_e[i] = summary(model)$sigma
}
```


**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

Confidence interval of $\hat{\beta}_1$:
\[
  \hat{\beta}_1 \pm t_{\alpha/2, n-2}\cdot \frac {s_e}{\sqrt{S_{xx}}}
\]

```{r}
critical_value = -qt (0.025, n-2)
S_xx = sum((x - mean(x))^2)
lower_95 = beta_hat_1 - critical_value * s_e/sqrt(S_xx)
upper_95 = beta_hat_1 + critical_value * s_e/sqrt(S_xx)
```


**(c)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
mean(lower_95 < 2 & 2 < upper_95)
```

94.68% of intervals contains the true $\beta_1$.


**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

```{r}
1 - mean(lower_95 < 0 & 0 < upper_95)
```

67.04% of the simulations will reject the test.


**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

```{r}
critical_value_99 = -qt (0.005, n-2)
S_xx = sum((x - mean(x))^2)
lower_99 = beta_hat_1 - critical_value_99 * s_e/sqrt(S_xx)
upper_99 = beta_hat_1 + critical_value_99 * s_e/sqrt(S_xx)
```


**(f)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
mean(lower_99 < 2 & 2 < upper_99)
```

98.8% of intervals contains the true $\beta_1$.

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

```{r}
1-mean(lower_99 < 0 & 0 < upper_99)
```

39.76% of the simulations will reject the test.


***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

```{r}
calc_pred_int = function (model, newdata, level = 0.95){
  x = as.vector (model$model[,2])
  S_xx = sum((x - mean(x))^2)
  s_e = summary(model)$sigma
  n = length(x)
  critical_value = -qt((1-level)/2 , df = n-2)
  est = coef(model)[[1]] + coef(model)[[2]] * newdata[[1]]
  lower_val = est - critical_value * s_e * sqrt (1 + 1/n + (newdata[[1]] - mean (x))^2 / S_xx)
  upper_val = est + critical_value * s_e * sqrt (1 + 1/n + (newdata[[1]] - mean (x))^2 / S_xx)
  c(estimate = est, lower = lower_val , upper = upper_val)
}
```


**(b)** After writing the function, run this code:

```{r}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```

**(c)** After writing the function, run this code:

```{r}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.90)
```


