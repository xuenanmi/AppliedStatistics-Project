---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, Xuenan Mi, NetID: xmi4"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Directions

This is an **individual** project. This is NOT like a homework assignment. This is NOT an assignment where collaboration is permissible. Discussion of question intent, coding problems/issues, and project administration may be discussed on the message board on a limited basis. However, sharing, copying, or providing any part of this project to another student is an infraction of the University’s rules on academic integrity. Any violation will be punished as severely as possible.

- Your project must be submitted through Coursera. You are required to upload one `.zip` file, named `yourNetID-sim-proj.zip`, which contains:
  + Your RMarkdown file which should be saved as `yourNetID-sim-proj.Rmd`.
  + The result of knitting your RMarkdown file as `yourNetID-sim-proj.html`.
  + Any outside data provided as a `.csv` file. (In this case, `study_1.csv` and `study_2.csv`.)
- Your `.Rmd` file should be written such that, when stored in a folder with any data you are asked to import, it will knit properly without modification. If your `.zip` file is organized properly, this should not be an issue.
- Include your name and NetID in the final document, not only in your filenames.

This project consists of **three** simulation studies. Unlike a homework assignment, these "exercises" are not broken down into parts (e.g., a, b, c), and so your analysis will not be similarly partitioned. Instead, your document should be organized more like a true project report, and it should use the overall format:

- Simulation Study 1
- Simulation Study 2
- Simulation Study 3

Within each of the simulation studies, you should use the format:

- Introduction
- Methods
- Results
- Discussion

The **introduction** section should relay what you are attempting to accomplish. It should provide enough background to your work such that a reader would not need this directions document to understand what you are doing. (Basically, assume the reader is mostly familiar with the concepts from the course, but not this project.)

The **methods** section should contain the majority of your “work.” This section will contain the bulk of the `R` code that is used to generate the results. Your `R` code is not expected to be perfect idiomatic `R`, but it is expected to be understood by a reader without too much effort. Use RMarkdown and code comments to your advantage to explain your code if needed.

The **results** section should contain numerical or graphical summaries of your results as they pertain to the goal of each study.

The **discussion** section should contain discussion of your results. The discussion section should contain discussion of your results. Potential topics for discussion are suggested at the end of each simulation study section, but they are not meant to be an exhaustive list. These simulation studies are meant to be explorations into the principles of statistical modeling, so do not limit your responses to short, closed form answers as you do in homework assignments. Use the potential discussion questions as a starting point for your response.

- Your resulting `.html` file will be considered a self-contained “report,” which is the material that will determine the majority of your grade. Be sure to visibly include all `R` code and output that is *relevant*. (You should not include irrelevant code you tried that resulted in error or did not answer the question correctly.)
- Grading will be based on a combination of completing the required tasks, discussion of results, `R` usage, RMarkdown usage, and neatness and organization. For full details see the provided rubric.
- At the beginning of *each* of the three simulation studies, set a seed equal to your birthday, as is done on homework. (It should be the first code run for each study.) These should be the only three times you set a seed.


# Simulation Study 1: Significance of Regression

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, use $2000$ simulations. For each simulation, fit a regression model of the same form used to perform the simulation.

Use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Done correctly, you will have simulated the `y` vector $2 (models)×3 (sigmas)×2000 (sims)=12000$ times.

Potential discussions:

- Do we know the true distribution of any of these values?
- How do the empirical distributions from the simulations compare to the true distributions? (You could consider adding a curve for the true distributions if you know them.)
- How are each of the $F$ statistic, the p-value, and $R^2$ related to $\sigma$? Are any of those relationships the same for the significant and non-significant models?

Additional things to consider:

- Organize the plots in a grid for easy comparison.




**Answer**

**Introduction**

In the simulation study1, through simulating two different models, we would like to investigate the significance of regression test. One model is **"significant"** model, $Y_i = 3 + 1* x_{i1} + 2* x_{i2} + 3* x_{i3} + \epsilon_i$, another model is **"non-significant"** model, $Y_i = 3 + \epsilon_i$, and $\epsilon_i \sim N(0, \sigma^2)$.  In this simulation study, we consider sample size = 25, and $\sigma \in (1, 5, 10)$. So in total we have 6 models (2 model * 3 $\sigma$), and each model simulate for 2000 times. 

After getting the simulation data, we can obtain the empirical distribution of F statistics, p-value and $R^2$. By comparing the empirical distribution and true distribution, and comparing the difference between different models, we could get some conclusion for significant of regression test and observe the influence of $sigma$ on the F statistics, p value and $R^2$.


**Method**

```{r}
birthday = 19890722
set.seed(birthday)
```


```{r}
library(broom)
study1 <- read.csv('study_1.csv', header = T, stringsAsFactors = F)
n = 25
num_sim = 2000

##sigma = 1 simulation
sigma = 1
sig_model_sigma1_Fvalue = rep(0, num_sim)
sig_model_sigma1_pvalue = rep(0, num_sim)
sig_model_sigma1_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 1 * study1$x1 + 1 * study1$x2 + 1 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  sig_model_sigma1_Fvalue[i] = summary(fit)$fstatistic['value']
  sig_model_sigma1_Rsquare[i] = summary(fit)$r.squared
  sig_model_sigma1_pvalue[i] = glance(fit)$p.value
}

nonsig_model_sigma1_Fvalue = rep(0, num_sim)
nonsig_model_sigma1_pvalue = rep(0, num_sim)
nonsig_model_sigma1_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 0 * study1$x1 + 0 * study1$x2 + 0 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  nonsig_model_sigma1_Fvalue[i] = summary(fit)$fstatistic['value']
  nonsig_model_sigma1_Rsquare[i] = summary(fit)$r.squared
  nonsig_model_sigma1_pvalue[i] = glance(fit)$p.value
}

##sigma = 5 simulation
sigma = 5
sig_model_sigma5_Fvalue = rep(0, num_sim)
sig_model_sigma5_pvalue = rep(0, num_sim)
sig_model_sigma5_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 1 * study1$x1 + 1 * study1$x2 + 1 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  sig_model_sigma5_Fvalue[i] = summary(fit)$fstatistic['value']
  sig_model_sigma5_Rsquare[i] = summary(fit)$r.squared
  sig_model_sigma5_pvalue[i] = glance(fit)$p.value
}

nonsig_model_sigma5_Fvalue = rep(0, num_sim)
nonsig_model_sigma5_pvalue = rep(0, num_sim)
nonsig_model_sigma5_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 0 * study1$x1 + 0 * study1$x2 + 0 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  nonsig_model_sigma5_Fvalue[i] = summary(fit)$fstatistic['value']
  nonsig_model_sigma5_Rsquare[i] = summary(fit)$r.squared
  nonsig_model_sigma5_pvalue[i] = glance(fit)$p.value
}

##sigma = 10 simulation
sigma = 10
sig_model_sigma10_Fvalue = rep(0, num_sim)
sig_model_sigma10_pvalue = rep(0, num_sim)
sig_model_sigma10_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 1 * study1$x1 + 1 * study1$x2 + 1 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  sig_model_sigma10_Fvalue[i] = summary(fit)$fstatistic['value']
  sig_model_sigma10_Rsquare[i] = summary(fit)$r.squared
  sig_model_sigma10_pvalue[i] = glance(fit)$p.value
}

nonsig_model_sigma10_Fvalue = rep(0, num_sim)
nonsig_model_sigma10_pvalue = rep(0, num_sim)
nonsig_model_sigma10_Rsquare = rep(0, num_sim)
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study1$y = 3 + 0 * study1$x1 + 0 * study1$x2 + 0 * study1$x3 + eps
  fit = lm(y ~ x1 + x2 + x3, data = study1)
  nonsig_model_sigma10_Fvalue[i] = summary(fit)$fstatistic['value']
  nonsig_model_sigma10_Rsquare[i] = summary(fit)$r.squared
  nonsig_model_sigma10_pvalue[i] = glance(fit)$p.value
}
 
```

**Result**

Make plots for empirical distribution of F statistics value from 6 models, including 3 significant models with different $\sigma \in (1, 2, 4)$ value and 3 non-significant models with different $\sigma \in (1, 2, 4)$ value. 
And F statistics should follow F-distribution with df1 = p-1, df2 = n-p.
- Significant model, p = 4, n = 25, so df1 = p-1 = 3, df2 = 21

```{r, fig.height=10, fig.width=8}

par(mfrow = c(3,2))
hist(sig_model_sigma1_Fvalue, breaks = 20, prob = T, main = expression(paste('Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'F value' , ylim = c(0, 0.8), xlim= c(0, 120))
curve(df(x, df1 = 3,df2 =21 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma1_Fvalue, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'F value')
hist(sig_model_sigma5_Fvalue, breaks = 20, prob = T, main = expression(paste('Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'F value', ylim = c(0, 0.8))
curve(df(x, df1 = 3,df2 =21 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma5_Fvalue, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'F value')
hist(sig_model_sigma10_Fvalue, breaks = 20, prob = T, main = expression(paste('Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'F value', ylim = c(0, 0.8))
curve(df(x, df1 = 3,df2 =21 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma10_Fvalue, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'F value')

```

Make plots for empirical distribution of p value from 6 models, including 3 significant models with different $\sigma \in (1, 2, 4)$ value and 3 non-significant models with different $\sigma \in (1, 2, 4)$ value. 
And p value should follow uniform distribution on (0, 1).

```{r, fig.height=10, fig.width=8}
par(mfrow = c(3,2))
hist(sig_model_sigma1_pvalue, breaks = 100, prob = T, main = expression(paste('Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma1_pvalue, breaks = 100, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)
hist(sig_model_sigma5_pvalue, breaks = 100, prob = T, main = expression(paste('Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma5_pvalue, breaks = 100, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)
hist(sig_model_sigma10_pvalue, breaks = 100, prob = T, main = expression(paste('Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma10_pvalue, breaks = 100, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'P value')
curve(dunif(x, 0, 1 ), col = 'darkorange', add = T, lwd = 3)

```

Make plots for empirical distribution of $R^2$ from 6 models, including 3 significant models with different $\sigma \in (1, 2, 4)$ value and 3 non-significant models with different $\sigma \in (1, 2, 4)$ value. 
And $R^2$ should follow Beta-distribution with shape1 = (p-1)/2, shape2 = (n-p)/2.
- Significant model, p = 4, n = 25, so shape1 = (p-1)/2 = 1.5, shape2 = (n-p)/2 = 10.5

```{r, fig.height=10, fig.width=8}
par(mfrow = c(3,2))
#R square follow beta distribution
#parameter of beta distribution 
shape1 = (4-1)/2
shape2 = (25-4)/2
hist(sig_model_sigma1_Rsquare, breaks = 20, prob = T,main = expression(paste('Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'R square', xlim = c(0,1))
curve(dbeta(x, shape1,shape2 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma1_Rsquare, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=1')), border = 'blue', xlab = 'R square')
hist(sig_model_sigma5_Rsquare, breaks = 20, prob = T, main = expression(paste('Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'R square', ylim = c(0,8))
curve(dbeta(x, shape1,shape2 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma5_Rsquare, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=5')), border = 'blue', xlab = 'R square')
hist(sig_model_sigma10_Rsquare, breaks = 20, prob = T, main = expression(paste('Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'R square', ylim = c(0,8))
curve(dbeta(x, shape1,shape2 ), col = 'darkorange', add = T, lwd = 3)
hist(nonsig_model_sigma10_Rsquare, breaks = 20, prob = T, main = expression(paste('Non-Significant Model with ', sigma, '=10')), border = 'blue', xlab = 'R square')
```


**Discussion**

- The true distribution of F statistics is F-distribution, with df1 = p-1, df2 = n-p. For significant models, p = 4, n = 25, so true distribution of F statistics is df1 = 3, df2 = 21. For non-significant model, there is only constant $\beta_0$ in the model, so it cannot follow F distribution. Based on the histogram of F statistics and curve of true distribution, we can see for significant model with $\sigma$ = 5 or 10, the empirical distribution of F statistics matches the true distribution. For significant model with $\sigma$ = 1, most F statistics values are large, so the F statistics value locate at the tail of F distribution. As the $\sigma$ increases, distribution of F statistics shift to left. That means $\sigma$ =10, F statistics trend to smaller value, $\sigma$ =1, F statistics trend to larger value.

- The true distribution of p value is uniform distribution on [0, 1]. For 3 different significant models, p value don't follow the uniform distribution. When $\sigma = 1$, all p value < 0.05; $\sigma =$ 5 or 10, some p value < 0.05. But for three non-significant models, p value almost follow the uniform distribution. As the $\sigma$ increases, p value trends to larger. That means $\sigma$ =10, p value trend to larger value, $\sigma$ =1,  p value trend to smaller value. Non-significant models don't have this trend.

- The true distribution of $R^2$ is Beta-distribution, with shape1 = (p-1)/2, shape2 = (n-p)/2. For significant models, p = 4, n = 25, so true distribution of $R^2$ is shape1 = (p-1)/2 = 1.5 , shape2 = (n-p)/2 = 10.5. For non-significant model, there is only constant $\beta_0$ in the model, so it cannot follow Beta-distribution. For significant model with $\sigma$ = 1, most $R^2$ are large, so $R^2$ value locate at the tail of beta-distribution. For significant model with $\sigma$ = 5 or 10, the empirical distribution of $R^2$ matches the true distribution.  As the $\sigma$ increases, distribution of $R^2$ shift to left. That means $\sigma$ =10, $R^2$ value trend to smaller value, $\sigma$ =1, $R^2$ value trend to larger value.




# Simulation Study 2: Using RMSE for Selection?

In homework we saw how Test RMSE can be used to select the “best” model. In this simulation study we will investigate how well this procedure works. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time you simulate the data, randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

Repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. For each value of $\sigma$, create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size. Also show the number of times the model of each size was chosen for each value of $\sigma$.

Done correctly, you will have simulated the $y$ vector $3×1000=3000$ times. You will have fit $9×3×1000=27000$ models. A minimal result would use $3$ plots. Additional plots may also be useful.

Potential discussions:

- Does the method **always** select the correct model? On average, does is select the correct model?
- How does the level of noise affect the results?


**Answer**

**Introduction**

When evaluating a model for prediction, we often use RMSE as a measure of how well the model is. But if we use all data to fit the model, we will notice as models larger, RMSE will always decrease. To correct this issue, we just use a portion of the data to fit the model, and use the remaining data to evaluate the model, we call these two kind data set as train data and test data, seprately. 

In this simulation project, we would like to investigate whether RMSE always can help us to select correct model. Here we consider data with 500 samples, and known true model with 6 predictors. For simulation, each time we randomly spilt the data into train and test sets of equal sizes, and fit 9 models, for each model, we calculate the train RMSE and test RMSE. We will repeat the process for 1000 times and use 3 different noise level, $\sigma \in (1, 2, 4)$. 

Through the simulations, we would like to see whether the RMSE always can help select the correct model, or on average, RMSE can select the correct one.

**Method**

Based on the data in the study2.csv, randomly split data into train and test sets with equal size, and the fit nine models. For each model, we calculate the Train RMSE and Test RMSE. And we repeat the process 1000 times for each value of $\sigma \in (1, 2, 4)$.

```{r}
birthday = 19890722
set.seed(birthday)
```

**Simulation at $\sigma = 1$**

```{r}
study2 <- read.csv('study_2.csv', header = T, stringsAsFactors = F)
n = 500
num_sim = 1000
beta_0 = 0
beta_1 = 3
beta_2 = -4
beta_3 = 1.6
beta_4 = -1.1
beta_5 = 0.7
beta_6 = 0.5


rmse  = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

for (j in c(1,2,4)){
  assign(paste0('sigma',j,'_RMSE', sep = ''), data.frame(
    mod1_trn = rep (0, num_sim), 
    mod2_trn = rep (0, num_sim), 
    mod3_trn = rep (0, num_sim),
    mod4_trn = rep (0, num_sim),
    mod5_trn = rep (0, num_sim),
    mod6_trn = rep (0, num_sim),
    mod7_trn = rep (0, num_sim),
    mod8_trn = rep (0, num_sim),
    mod9_trn = rep (0, num_sim),
    mod1_tst = rep (0, num_sim), 
    mod2_tst = rep (0, num_sim), 
    mod3_tst = rep (0, num_sim),
    mod4_tst = rep (0, num_sim),
    mod5_tst = rep (0, num_sim),
    mod6_tst = rep (0, num_sim),
    mod7_tst = rep (0, num_sim),
    mod8_tst = rep (0, num_sim),
    mod9_tst = rep (0, num_sim)))
}  
  
#sigma =1 simulation
sigma =1
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study2$y = beta_0 + beta_1 * study2$x1 + beta_2 * study2$x2 + beta_3 * study2$x3 + beta_4 * study2$x4 + beta_5 * study2$x5 + beta_6 * study2$x6 + eps
  trn_idx = sample(1:nrow(study2), 250)
  trn = study2[trn_idx,]
  tst = study2[-trn_idx,]
  model1<-lm(y ~ x1, data = trn)
  model2<-lm(y ~ x1 + x2, data = trn)
  model3<-lm(y ~ x1 + x2 + x3, data = trn)
  model4<-lm(y ~ x1 + x2 + x3 + x4, data = trn)
  model5<-lm(y ~ x1 + x2 + x3 + x4 + x5, data = trn)
  model6<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = trn)
  model7<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = trn)
  model8<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = trn)
  model9<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = trn)
  
  sigma1_RMSE$mod1_trn[i] = rmse(trn$y, predict(model1, trn))
  sigma1_RMSE$mod2_trn[i] = rmse(trn$y, predict(model2, trn))
  sigma1_RMSE$mod3_trn[i] = rmse(trn$y, predict(model3, trn))
  sigma1_RMSE$mod4_trn[i] = rmse(trn$y, predict(model4, trn))
  sigma1_RMSE$mod5_trn[i] = rmse(trn$y, predict(model5, trn))
  sigma1_RMSE$mod6_trn[i] = rmse(trn$y, predict(model6, trn))
  sigma1_RMSE$mod7_trn[i] = rmse(trn$y, predict(model7, trn))
  sigma1_RMSE$mod8_trn[i] = rmse(trn$y, predict(model8, trn))
  sigma1_RMSE$mod9_trn[i] = rmse(trn$y, predict(model9, trn))
  
  sigma1_RMSE$mod1_tst[i] = rmse(tst$y, predict(model1, tst))
  sigma1_RMSE$mod2_tst[i] = rmse(tst$y, predict(model2, tst))
  sigma1_RMSE$mod3_tst[i] = rmse(tst$y, predict(model3, tst))
  sigma1_RMSE$mod4_tst[i] = rmse(tst$y, predict(model4, tst))
  sigma1_RMSE$mod5_tst[i] = rmse(tst$y, predict(model5, tst))
  sigma1_RMSE$mod6_tst[i] = rmse(tst$y, predict(model6, tst))
  sigma1_RMSE$mod7_tst[i] = rmse(tst$y, predict(model7, tst))
  sigma1_RMSE$mod8_tst[i] = rmse(tst$y, predict(model8, tst))
  sigma1_RMSE$mod9_tst[i] = rmse(tst$y, predict(model9, tst))
}

```

**Simulation at $\sigma = 2$**

```{r}
#sigma = 2 simulation
sigma = 2
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study2$y = beta_0 + beta_1 * study2$x1 + beta_2 * study2$x2 + beta_3 * study2$x3 + beta_4 * study2$x4 + beta_5 * study2$x5 + beta_6 * study2$x6 + eps
  trn_idx = sample(1:nrow(study2), 250)
  trn = study2[trn_idx,]
  tst = study2[-trn_idx,]
  model1<-lm(y ~ x1, data = trn)
  model2<-lm(y ~ x1 + x2, data = trn)
  model3<-lm(y ~ x1 + x2 + x3, data = trn)
  model4<-lm(y ~ x1 + x2 + x3 + x4, data = trn)
  model5<-lm(y ~ x1 + x2 + x3 + x4 + x5, data = trn)
  model6<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = trn)
  model7<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = trn)
  model8<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = trn)
  model9<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = trn)
  
  sigma2_RMSE$mod1_trn[i] = rmse(trn$y, predict(model1, trn))
  sigma2_RMSE$mod2_trn[i] = rmse(trn$y, predict(model2, trn))
  sigma2_RMSE$mod3_trn[i] = rmse(trn$y, predict(model3, trn))
  sigma2_RMSE$mod4_trn[i] = rmse(trn$y, predict(model4, trn))
  sigma2_RMSE$mod5_trn[i] = rmse(trn$y, predict(model5, trn))
  sigma2_RMSE$mod6_trn[i] = rmse(trn$y, predict(model6, trn))
  sigma2_RMSE$mod7_trn[i] = rmse(trn$y, predict(model7, trn))
  sigma2_RMSE$mod8_trn[i] = rmse(trn$y, predict(model8, trn))
  sigma2_RMSE$mod9_trn[i] = rmse(trn$y, predict(model9, trn))
  
  sigma2_RMSE$mod1_tst[i] = rmse(tst$y, predict(model1, tst))
  sigma2_RMSE$mod2_tst[i] = rmse(tst$y, predict(model2, tst))
  sigma2_RMSE$mod3_tst[i] = rmse(tst$y, predict(model3, tst))
  sigma2_RMSE$mod4_tst[i] = rmse(tst$y, predict(model4, tst))
  sigma2_RMSE$mod5_tst[i] = rmse(tst$y, predict(model5, tst))
  sigma2_RMSE$mod6_tst[i] = rmse(tst$y, predict(model6, tst))
  sigma2_RMSE$mod7_tst[i] = rmse(tst$y, predict(model7, tst))
  sigma2_RMSE$mod8_tst[i] = rmse(tst$y, predict(model8, tst))
  sigma2_RMSE$mod9_tst[i] = rmse(tst$y, predict(model9, tst))
}
```

**Simulation at $\sigma = 4$**

```{r}
#sigma = 4 simulation
sigma = 4
for (i in 1:num_sim){
  eps = rnorm(n, 0, sigma)
  study2$y = beta_0 + beta_1 * study2$x1 + beta_2 * study2$x2 + beta_3 * study2$x3 + beta_4 * study2$x4 + beta_5 * study2$x5 + beta_6 * study2$x6 + eps
  trn_idx = sample(1:nrow(study2), 250)
  trn = study2[trn_idx,]
  tst = study2[-trn_idx,]
  model1<-lm(y ~ x1, data = trn)
  model2<-lm(y ~ x1 + x2, data = trn)
  model3<-lm(y ~ x1 + x2 + x3, data = trn)
  model4<-lm(y ~ x1 + x2 + x3 + x4, data = trn)
  model5<-lm(y ~ x1 + x2 + x3 + x4 + x5, data = trn)
  model6<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = trn)
  model7<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = trn)
  model8<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = trn)
  model9<-lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = trn)
  
  sigma4_RMSE$mod1_trn[i] = rmse(trn$y, predict(model1, trn))
  sigma4_RMSE$mod2_trn[i] = rmse(trn$y, predict(model2, trn))
  sigma4_RMSE$mod3_trn[i] = rmse(trn$y, predict(model3, trn))
  sigma4_RMSE$mod4_trn[i] = rmse(trn$y, predict(model4, trn))
  sigma4_RMSE$mod5_trn[i] = rmse(trn$y, predict(model5, trn))
  sigma4_RMSE$mod6_trn[i] = rmse(trn$y, predict(model6, trn))
  sigma4_RMSE$mod7_trn[i] = rmse(trn$y, predict(model7, trn))
  sigma4_RMSE$mod8_trn[i] = rmse(trn$y, predict(model8, trn))
  sigma4_RMSE$mod9_trn[i] = rmse(trn$y, predict(model9, trn))
  
  sigma4_RMSE$mod1_tst[i] = rmse(tst$y, predict(model1, tst))
  sigma4_RMSE$mod2_tst[i] = rmse(tst$y, predict(model2, tst))
  sigma4_RMSE$mod3_tst[i] = rmse(tst$y, predict(model3, tst))
  sigma4_RMSE$mod4_tst[i] = rmse(tst$y, predict(model4, tst))
  sigma4_RMSE$mod5_tst[i] = rmse(tst$y, predict(model5, tst))
  sigma4_RMSE$mod6_tst[i] = rmse(tst$y, predict(model6, tst))
  sigma4_RMSE$mod7_tst[i] = rmse(tst$y, predict(model7, tst))
  sigma4_RMSE$mod8_tst[i] = rmse(tst$y, predict(model8, tst))
  sigma4_RMSE$mod9_tst[i] = rmse(tst$y, predict(model9, tst))
}
```

**Results**

For each value of $\sigma \in (1, 2, 4)$, we calculate the average RMSE of Train data set and Test data set for nine models, and make plot for these values.

```{r, fig.height=10, fig.width=8}
library(ggplot2)
library(pdp)
library(knitr)
Model = rep(c("model1",'model2','model3','model4','model5','model6','model7','model8','model9'), 2)
sigma1_mean= apply(sigma1_RMSE, 2, mean)
sigma1_mean_RMSE = data.frame(Model, Data = c(rep('Train',9),  rep('Test',9)) ,Avg_RMSE = sigma1_mean)
row.names(sigma1_mean_RMSE) = NULL

sigma2_mean= apply(sigma2_RMSE, 2, mean)
sigma2_mean_RMSE = data.frame(Model, Data = c(rep('Train',9),  rep('Test',9)) ,Avg_RMSE = sigma2_mean)
row.names(sigma2_mean_RMSE) = NULL

sigma4_mean= apply(sigma4_RMSE, 2, mean)
sigma4_mean_RMSE = data.frame(Model, Data = c(rep('Train',9),  rep('Test',9)) ,Avg_RMSE = sigma4_mean)
row.names(sigma4_mean_RMSE) = NULL

#create table for average RMSE of Train and Test
sigma1_mean_RMSE_wide =reshape(sigma1_mean_RMSE, idvar = "Model", timevar = "Data", direction = 'wide')
sigma2_mean_RMSE_wide =reshape(sigma2_mean_RMSE, idvar = "Model", timevar = "Data", direction = 'wide')
sigma4_mean_RMSE_wide =reshape(sigma4_mean_RMSE, idvar = "Model", timevar = "Data", direction = 'wide')

RMSE_table = cbind(sigma1_mean_RMSE_wide, sigma2_mean_RMSE_wide[,c(2,3)], sigma4_mean_RMSE_wide[,c(2,3)])
colnames(RMSE_table) = c('Model', 'Train_RMSE(Sigma = 1)', 'Test_RMSE(Sigma = 1)', 'Train_RMSE(Sigma = 2)', 'Test_RMSE(Sigma = 2)', 'Train_RMSE(Sigma = 4)', 'Test_RMSE(Sigma = 4)')
kable(RMSE_table)

p1 = ggplot(data=sigma1_mean_RMSE, aes(x=Model, y=Avg_RMSE, fill=Data)) + geom_bar(stat="identity", position=position_dodge()) + ggtitle(expression(paste('Average RMSE vs Model Size at ', sigma, '= 1'))) + theme(plot.title = element_text(hjust = 0.5))
p2 = ggplot(data=sigma2_mean_RMSE, aes(x=Model, y=Avg_RMSE, fill=Data)) + geom_bar(stat="identity", position=position_dodge()) + ggtitle(expression(paste('Average RMSE vs Model Size at ', sigma, '= 2'))) + theme(plot.title = element_text(hjust = 0.5))
p3 = ggplot(data=sigma4_mean_RMSE, aes(x=Model, y=Avg_RMSE, fill=Data)) + geom_bar(stat="identity", position=position_dodge()) + ggtitle(expression(paste('Average RMSE vs Model Size at ', sigma, '= 4'))) + theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p1, p2, p3, nrow = 3)

```

From the above table and plots, on average we could select the correct model (model 6) based on the average test RMSE. But we don't know whether each time of 1000 simulations select the correct model, so we find the correct model based on the test RMSE of each simulation and calculate the frequency of the model as the correct model.

```{r, fig.height=10, fig.width=8}
sigma1_RMSE_test<- sigma1_RMSE[,c(10:18)]
df1 <- as.data.frame(table(colnames(sigma1_RMSE_test)[apply(sigma1_RMSE_test,1,which.min)]))
df_sigma1<- data.frame(
  Model = c('model1', 'model2','model3', 'model4','model5', 'model6','model7', 'model8','model9'),
  Freq = c(rep(0,5), df1$Freq)
)
sigma2_RMSE_test<- sigma2_RMSE[,c(10:18)]
df2 <- as.data.frame(table(colnames(sigma2_RMSE_test)[apply(sigma2_RMSE_test,1,which.min)]))
df_sigma2<- data.frame(
  Model = c('model1', 'model2','model3', 'model4','model5', 'model6','model7', 'model8','model9'),
  Freq = c(rep(0,2), df2$Freq)
)
sigma4_RMSE_test<- sigma4_RMSE[,c(10:18)]
df4 <- as.data.frame(table(colnames(sigma4_RMSE_test)[apply(sigma4_RMSE_test,1,which.min)]))
df_sigma4<- data.frame(
  Model = c('model1', 'model2','model3', 'model4','model5', 'model6','model7', 'model8','model9'),
  Freq = c(0, df4$Freq)
)

p4<-ggplot(data=df_sigma1, aes(x=Model, y=Freq)) +geom_bar(stat="identity", fill="steelblue") + geom_text(aes(label=Freq), vjust=-0.3, size=3.5) + ggtitle(expression(paste('Frequency of model selection vs Model Size at ', sigma, '= 1'))) + theme(plot.title = element_text(hjust = 0.5))
p5<-ggplot(data=df_sigma2, aes(x=Model, y=Freq)) +geom_bar(stat="identity", fill="steelblue") + geom_text(aes(label=Freq), vjust=-0.3, size=3.5) + ggtitle(expression(paste('Frequency of model selection vs Model Size at ', sigma, '= 2'))) + theme(plot.title = element_text(hjust = 0.5))
p6<-ggplot(data=df_sigma4, aes(x=Model, y=Freq)) +geom_bar(stat="identity", fill="steelblue") + geom_text(aes(label=Freq), vjust=-0.3, size=3.5) + ggtitle(expression(paste('Frequency of model selection vs Model Size at ', sigma, '= 4'))) + theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p4, p5, p6, nrow = 3)

```



**Discussion**

- From the simulation results, we found for each value of $\sigma \in (1, 2, 4)$, the model with more predictors always has the smaller train RMSE. Train RMSE use the model fit to the training data, and evaluated on the used train data set, so train RMSE alwasys go down as the complexity of a linear model increases. Thus we cannot use train RMSE to comparing models.
On the other side, Test RMSE uses the model fit to the train data set, but evaluated on the unused test data set. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model. Thus it's more reasonable to select model based on test RMSE. In our simulation results, for each value of $\sigma \in (1, 2, 4)$, model 6 (the correct model) has the lowest average test RMSE. 

- On average, test RMSE can help us select the correct model. But among 1000 simulations, test RMSE cannot always select the correct model. At $\sigma = 1$, among 1000 simulation, we select the correct model for 560 times. At $\sigma = 2$, among 1000 simulation, we select the correct model for 511 times. At $\sigma = 4$, among 1000 simulation, we select the correct model for 346 times. As the $\sigma$ increases, the probability of selecting the correct model decreases. 

- As the $\sigma$ increases, the train RMSE and test RMSE increase. For models with same predictors, simulation at $\sigma = 4$ has the largest train RMSE and test RMSE. And based on the plot, as the $\sigma$ increases, the variation in average RMSE of nine models decrease. For simulation at $\sigma = 4$, the variation in average RMSE of nine models is smallest. 




# Simulation Study 3: Power (Graduate Students only)

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

```{r eval=FALSE}
#x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

It is *possible* to derive an expression for power mathematically, but often this is difficult, so instead, we rely on simulation.

Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?



**Answers**

**Introduction**

Power is an important value for evaluating a simple liear regression model. In this simulation study, we would like to investigate the power of the significance of regression test. We already know the definition of power of regression model, that is, the probability of rejecting the null hypothesis when the null is not true. When power is high, that means we could avoid the Type 2 error. So we hope the model has a higher power. 

There are several factors could affect the power value, including sample size, $\beta_1$, $\sigma$, and $\alpha$. In this project, we will through the simulation to investigate the influence of first three factors on power. 

Here, we use a simple linear model without $\beta_0$. The signal strength $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$, $\sigma \in (1, 2, 4)$,sample size $n \in (10, 20, 30)$. And we hold $\alpha$ = 0.05. 


**Method**

For each possible $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$ and $\sigma \in (1, 2, 4)$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations for different sample size $n \in (10, 20, 30)$, and at $\alpha = 0.05$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

**Simulation at $\sigma = 1$**

```{r}
birthday = 19890722
set.seed(birthday)
```

```{r}
beta_1 = seq(-2, 2, 0.1)
num_sim = 1000

#sigma = 1 simulation
sigma = 1
power_sigma1 = data.frame(sample10 = rep(0,length(beta_1)), sample20 = rep(0,length(beta_1)), sample30 = rep(0,length(beta_1)))
for (k in 1:3){
  n = 10*k
  x_values = seq(0, 5, length = n)
  power_df = rep(0, length(beta_1))
  for (j in 1:length(beta_1)){
    pvalue = rep(0, num_sim)
    for (i in 1:num_sim){
       eps = rnorm(n, 0, sigma)
       y = beta_1[j] * x_values + eps
       fit = lm(y ~ x_values)
       pvalue[i] = summary(fit)$coefficients[2,4]
       }
     power_df[j] = mean(pvalue < 0.05)
  }
  power_sigma1[,k] = power_df 
}

```

**Simulation at $\sigma = 2$**

```{r}
#sigma = 2 simulation
sigma = 2
power_sigma2 = data.frame(sample10 = rep(0,length(beta_1)), sample20 = rep(0,length(beta_1)), sample30 = rep(0,length(beta_1)))
for (k in 1:3){
  n = 10*k
  x_values = seq(0, 5, length = n)
  power_df = rep(0, length(beta_1))
  for (j in 1:length(beta_1)){
    pvalue = rep(0, num_sim)
    for (i in 1:num_sim){
       eps = rnorm(n, 0, sigma)
       y = beta_1[j] * x_values + eps
       fit = lm(y ~ x_values)
       pvalue[i] = summary(fit)$coefficients[2,4]
       }
     power_df[j] = mean(pvalue < 0.05)
  }
  power_sigma2[,k] = power_df 
}
```

**Simulation at $\sigma = 4$**

```{r}
#sigma = 4 simulation
sigma = 4
power_sigma4 = data.frame(sample10 = rep(0,length(beta_1)), sample20 = rep(0,length(beta_1)), sample30 = rep(0,length(beta_1)))
for (k in 1:3){
  n = 10*k
  x_values = seq(0, 5, length = n)
  power_df = rep(0, length(beta_1))
  for (j in 1:length(beta_1)){
    pvalue = rep(0, num_sim)
    for (i in 1:num_sim){
       eps = rnorm(n, 0, sigma)
       y = beta_1[j] * x_values + eps
       fit = lm(y ~ x_values)
       pvalue[i] = summary(fit)$coefficients[2,4]
       }
     power_df[j] = mean(pvalue < 0.05)
  }
  power_sigma4[,k] = power_df 
}
```



**Results**

Make plot for each value of $\sigma\in (1, 2, 4)$, and in each plot, for each value of n, we will show how power is affected by signal strength $\beta_1$.


```{r, fig.height=10, fig.width=8}
par(mfrow = c(3,1))
#plot for sigma = 1
plot(beta_1, power_sigma1$sample10, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = expression(paste('Power vs Signal Strength for ', sigma, " = 1")))
lines(beta_1, power_sigma1$sample10, type ='o', col = 'blue', cex = 0.5, lty =1 )
lines(beta_1, power_sigma1$sample20, type ='o', col = 'red', cex = 0.5, lty =2)
lines(beta_1, power_sigma1$sample30, type ='o', col = 'orange', cex = 0.5, lty =3)
legend('bottomright', c('n = 10', 'n = 20', 'n = 30'),lty = c(1,2,3), col = c('blue','red','orange'))

#plot for sigma = 2
plot(beta_1, power_sigma2$sample10, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = expression(paste('Power vs Signal Strength for ', sigma, " = 2")))
lines(beta_1, power_sigma2$sample10, type ='o', col = 'blue', cex = 0.5, lty =1 )
lines(beta_1, power_sigma2$sample20, type ='o', col = 'red', cex = 0.5, lty =2)
lines(beta_1, power_sigma2$sample30, type ='o', col = 'orange', cex = 0.5, lty =3)
legend('bottomright', c('n = 10', 'n = 20', 'n = 30'),lty = c(1,2,3), col = c('blue','red','orange'))

#plot for sigma = 4
plot(beta_1, power_sigma4$sample10, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = expression(paste('Power vs Signal Strength for ', sigma, " = 4")))
lines(beta_1, power_sigma4$sample10, type ='o', col = 'blue', cex = 0.5, lty =1 )
lines(beta_1, power_sigma4$sample20, type ='o', col = 'red', cex = 0.5, lty =2)
lines(beta_1, power_sigma4$sample30, type ='o', col = 'orange', cex = 0.5, lty =3)
legend('bottomright', c('n = 10', 'n = 20', 'n = 30'),lty = c(1,2,3), col = c('blue','red','orange'))
```



**Discussion**

Power is the probability of rejecting $H_0$ when $H_0$ is not true, sample size $n$, signal strength $\beta_1$ and noise level $\sigma$ and $\alpha$ can affect the power. In this project, the $\alpha$ level is 0.05, so we will discuss the influence of other three factors on the power.

- Sample Size

When $\beta_1$ and $\sigma$ are constant, sample size increases, the power increases. In each plot, plot of sample size = 30 has the largest power.

- Singal Strength, $\beta_1$

When sample size and $\sigma$ are constant, $\beta_1$ is further away from 0, the power is larger. For each plot in results, $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$, when $\beta_1 = 0$, the power is very close to 0, which is the smallest power. In other words, the greater the difference between the true value of $\beta_1$ and the value in null hypothesis ($\beta_1 = 0$), the greater the power of test. 

- Noise level, $\sigma$

To investigate how $\sigma$ affect the power, we need to keep sample size and $\beta_1$ are constant. So we put three different $\sigma$ in one figure, it's better for us to compare the influence of $\sigma$.

```{r, fig.height=10, fig.width=8}
par(mfrow = c(3,1))
#plot for n = 10
plot(beta_1, power_sigma1$sample10, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = 'Power vs Sample Size = 10')
lines(beta_1, power_sigma1$sample10, type ='o', col = 'black', cex = 0.5, lty =1 )
lines(beta_1, power_sigma2$sample10, type ='o', col = 'green', cex = 0.5, lty =2)
lines(beta_1, power_sigma4$sample10, type ='o', col = 'purple', cex = 0.5, lty =3)
legend('bottomright', c(expression(paste(sigma, ' = 1')), expression(paste(sigma, ' = 2')), expression(paste(sigma, ' = 4'))),lty = c(1,2,3), col = c('black','green','purple'))

#plot for n = 20
plot(beta_1, power_sigma1$sample20, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = 'Power vs Sample Size = 20')
lines(beta_1, power_sigma1$sample20, type ='o', col = 'black', cex = 0.5, lty =1 )
lines(beta_1, power_sigma2$sample20, type ='o', col = 'green', cex = 0.5, lty =2)
lines(beta_1, power_sigma4$sample20, type ='o', col = 'purple', cex = 0.5, lty =3)
legend('bottomright', c(expression(paste(sigma, ' = 1')), expression(paste(sigma, ' = 2')), expression(paste(sigma, ' = 4'))),lty = c(1,2,3), col = c('black','green','purple'))

#plot for n = 30
plot(beta_1, power_sigma1$sample30, ylab = 'Power', xlab = expression(beta[1]), col = 'blue', cex = 0.5, main = 'Power vs Sample Size = 30')
lines(beta_1, power_sigma1$sample30, type ='o', col = 'black', cex = 0.5, lty =1 )
lines(beta_1, power_sigma2$sample30, type ='o', col = 'green', cex = 0.5, lty =2)
lines(beta_1, power_sigma4$sample30, type ='o', col = 'purple', cex = 0.5, lty =3)
legend('bottomright', c(expression(paste(sigma, ' = 1')), expression(paste(sigma, ' = 2')), expression(paste(sigma, ' = 4'))),lty = c(1,2,3), col = c('black','green','purple'))

```

Based on the plots, when sample size and $\beta_1$ are constant, the greater $\sigma$, the smaller power of test. 

- I think 1000 simulations are sufficient. Based on the results from 1000 simulations, we already observe the influence of sample size, $\beta_1$ and $\sigma$ on the power of test, and among different combinations of $\beta_1$ , $\sigma$ and sample size, some power value of test reach to 1, which the largest value of power.

